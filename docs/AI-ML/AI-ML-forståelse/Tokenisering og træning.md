# **LÃ¦ringsside â€“ Uddybning til Etape 3 (Tokenisering & ModeltrÃ¦ning)**



## ğŸ§© **Hvad er tokenisering â€“ og hvorfor er det vigtigt?**

Tokenisering er processen hvor tekst bliver opdelt i mindre enheder, der kaldes _tokens_.  
Modellerne arbejder ikke med ord som vi kender dem, men med talrÃ¦kker som reprÃ¦senterer tokens.

Eksempel:  
Ordet _â€sunhedstilstandâ€_ kan blive delt i flere tokens:

- sund
    
- hed
    
- stil
    
- stand
    

Dette afhÃ¦nger af den algoritme modellen bruger.

Tokenisering er vigtig fordi:

- den bestemmer **hvordan modellen forstÃ¥r tekst**
    
- den pÃ¥virker **pris** (flere tokens = dyrere)
    
- den pÃ¥virker **kvalitet** (fagtermer â†’ bedre eller dÃ¥rligere delt)
    
- den pÃ¥virker **prÃ¦cision**, isÃ¦r i sundhedsdata
    

---

## ğŸ”§ **Byte Pair Encoding (BPE)**

En af de mest brugte algoritmer.

Princip:  
Start med helt smÃ¥ enheder (bogstaver), og slÃ¥ de kombinationer sammen der forekommer oftest.

Fordele:

- effektiv til mange sprog
    
- hÃ¥ndterer nye ord godt
    
- bruges i GPT, Claude, DeepSeek, Llama
    

Ulemper:

- faglige ord kan blive delt meget, hvilket gÃ¸r dem svÃ¦rere for modellen
    

---

## ğŸ”§ **WordPiece**

Anvendes bl.a. af BERT.

Princip:  
Finder subwords baseret pÃ¥ sandsynlighed i trÃ¦ningsdata.

Fordele:

- godt til generelle domÃ¦ner
    
- stabil struktur
    

Ulemper:

- medicinske eller sjÃ¦ldne ord splittes ofte ekstremt meget
    

---

## ğŸ”§ **SentencePiece**

En anden tokeniseringsmetode, ofte brugt i open-source modeller.

Fordele:

- kan arbejde uden mellemrum (â€œwhitespace-agnosticâ€)
    
- god til ikke-engelske sprog
    

---

## ğŸ§  **Hvorfor bruger LLMâ€™er subwords?**

Modellerne kan ikke lÃ¦re alle ord i alle sprog.  
Subwords er en lÃ¸sning:

- modellen lÃ¦rer stykker af ord
    
- kan kombinere dem til nye ord
    
- bedre generalisering
    
- fÃ¦rre tokens end bogstav-for-bogstav modeller
    

For sundhedsdomÃ¦net betyder det:

- nogle modeller forstÃ¥r medicinske ord bedre end andre
    
- ord som â€œbetahistineâ€ eller â€œlabyrintitisâ€ kan vÃ¦re Ã©t token eller fem
    
- dette pÃ¥virker forstÃ¥elsen i en chatbot
    

---

## ğŸ”¬ **Hvordan pÃ¥virker tokenisering modeltrÃ¦ning?**

NÃ¥r en model trÃ¦nes, lÃ¦rer den mÃ¸nstre baseret pÃ¥ tokens, ikke ord.

Hvis et vigtigt ord bliver delt op i mange subwords, sÃ¥ skal modellen:

- lÃ¦re mÃ¸nstre mellem flere tokens
    
- bruge mere kontekst
    
- bruge flere ressourcer
    
- har stÃ¸rre risiko for at misforstÃ¥ sammenhÃ¦nge
    

â†’ Derfor er nogle modeller bedre til sundhedsdata end andre.

---

## ğŸ’¸ **Tokenisering og pris**

Hvis du bruger API-modeller (OpenAI, Claude osv.):

- pris beregnes per token
    
- flere subwords = dyrere
    
- lange ord â†’ dyre forkerte modeller
    

OpenAI og Claude kan vÃ¦re dyrere, fordi deres tokenizers splitter dansk + medicinsk tekst mere aggressivt.

---

## ğŸ†š **Hvorfor forskellige modeller giver forskellige resultater**


Tokenisering spiller en rolle fordi:

- modeller er trÃ¦net pÃ¥ forskellige typer data
    
- deres tokenizere er optimeret til forskellige sprog
    
- subword-logikken pÃ¥virker hvordan de â€œforstÃ¥râ€ fagtermer
    
- encoder-modeller og decoder-modeller bruger nogle gange forskellige tokenizers
    

Dette forklarer hvorfor:

- nogle modeller er bedre til dansk
    
- nogle er bedre til medicinske termer
    
- nogle er bedre til komplekse brugerbeskeder
    
- prisen Ã¦ndrer sig fra model til model
    

